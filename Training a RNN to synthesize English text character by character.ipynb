{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"raining a RNN to synthesize English text character by character.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"st_40Wd1Mux0","colab_type":"text"},"source":["#Deep Learning in Data Science: **Training a RNN to synthesize English text character by character** \n","#Alexander Bea - abea@kth.se\n","#Assignment 4\n","\n","*In this assignment I had to train a vanilla RNN with outputs using the text from the book The Globlet of Fire by J.K. Rowling.*"]},{"cell_type":"markdown","metadata":{"id":"HRDkHMOjhb1i","colab_type":"text"},"source":["The following implementation will train a recurrent neural network (RNN) that shows how the evolution of the text synthesized by my RNN during training by inclduing a sample of synthesized text (200 characters long) before the first and before every 10,000th update steps when I train for 100,000 update. Furthermore, I also present 1000 characters with the best implementation."]},{"cell_type":"code","metadata":{"id":"2WVWiL1MFYLL","colab_type":"code","cellView":"form","outputId":"f83f4fa2-f91e-43be-8574-c36b9d5ee4d9","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Installers\n","!pip install texttable"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (1.6.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uU3FwIcgRLeB","colab_type":"code","cellView":"form","outputId":"118b7362-fffb-4101-9f2f-3074c5a2a648","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#@title Import libraries\n","from texttable import Texttable\n","from collections import OrderedDict\n","from keras import applications\n","from keras.models import Sequential\n","from keras.layers import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MkNd4fm5-I5y","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Read file from drive\n","def LoadData():\n","    data = open(\"/content/drive/My Drive/The Globlet of Fire by J.K. Rowling/goblet_book.txt\", \"r\", encoding='utf8').read()\n","    chars = list(set(data))\n","    data = {\"data\": data, \"chars\": chars,\n","            \"vocLen\": len(chars), \"getIndFromChar\": OrderedDict(\n","                (char, ix) for ix, char in enumerate(chars)),\n","            \"getCharFromInd\": OrderedDict((ix, char) for ix, char in\n","                enumerate(chars))}\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sV4ft36DUzCi","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Initialization\n","class RecurrentNeuralNetwork():\n","  def __init__(self, data, m=100, eta=.1, seq_length=25):\n","      self.m, self.eta, self.N = m, eta, seq_length\n","      for k, v in data.items():\n","          setattr(self, k, v)\n","      self.b, self.c, self.U, self.V, self.W = self._init_parameters(self.m, self.vocLen)\n","\n","  @staticmethod\n","  def _init_parameters(m, K, sig=0.01):\n","      c = np.zeros((K, 1))\n","      b = np.zeros((m, 1))\n","      V = np.random.normal(0, sig, size=(K, m))\n","      W = np.random.normal(0, sig, size=(m, m))\n","      U = np.random.normal(0, sig, size=(m, K))\n","      return b, c, U, V, W"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybq5xqxcVTuV","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Softmax, EvaluateClassifier\n","class RNN_With_Functions(RecurrentNeuralNetwork):\n","    @staticmethod\n","    def SoftMax(x):\n","        s = np.exp(x - np.max(x, axis=0)) / np.exp(x - np.max(x, axis=0)).sum(axis=0)\n","        return s\n","\n","    def EvaluateClassifier(self, h, x):\n","        a = self.W@h + self.U@x + self.b\n","        h = np.tanh(a)\n","        o = self.V@h + self.c\n","        p = self.SoftMax(o)\n","        return a, h, o, p\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CplOUxGJWMv6","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Synthesize Text\n","class RNN_Synthesizer(RNN_With_Functions):\n","    def SynthesizeText(self, h, aax, n):\n","        text = ''\n","        nxt = np.zeros((self.vocLen, 1))\n","        nxt[aax] = 1 \n","        for s in range(n):\n","            _, h, _, p = self.EvaluateClassifier(h, nxt)\n","            aax = np.random.choice(range(self.vocLen), p=p.flat)\n","            nxt = np.zeros((self.vocLen, 1))\n","            nxt[aax] = 1 \n","            text += self.getCharFromInd[aax]\n","        return text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CMQOQO0gOdp","colab_type":"text"},"source":["The following functions will compute the gradients analytically, numerically and check the gradietns. The difference between the two gradients computation will be iteratively presented when the training is ongoing."]},{"cell_type":"code","metadata":{"id":"BmE517cDWc6x","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Compute and Check Gradients\n","class RNN_Gradients(RNN_Synthesizer):\n","    def ComputeGradientsAnalytically(self, inputs, targets, hp):\n","        loss = 0\n","        aa, bb, cc, dd, ee = {}, {}, {}, {}, {}\n","        cc[-1] = np.copy(hp)\n","        nt = len(inputs)\n","        for t in range(nt):\n","            bb[t] = np.zeros((self.vocLen, 1))\n","            bb[t][inputs[t]] = 1 \n","            aa[t], cc[t], dd[t], ee[t] = self.EvaluateClassifier(cc[t-1], bb[t])\n","            loss += -np.log(ee[t][targets[t]][0]) \n","\n","        gradients = {\"W\": np.zeros_like(self.W), \"U\": np.zeros_like(self.U),\n","                 \"V\": np.zeros_like(self.V), \"b\": np.zeros_like(self.b),\n","                 \"c\": np.zeros_like(self.c), \"o\": np.zeros_like(ee[0]),\n","                 \"h\": np.zeros_like(cc[0]), \"hnxt\": np.zeros_like(cc[0]),\n","                 \"a\": np.zeros_like(aa[0])}\n","\n","        for t in reversed(range(nt)):\n","            gradients[\"o\"] = np.copy(ee[t])\n","            gradients[\"o\"][targets[t]] -= 1\n","            gradients[\"V\"] += gradients[\"o\"]@cc[t].T\n","            gradients[\"c\"] += gradients[\"o\"]\n","            gradients[\"h\"] = self.V.T@gradients[\"o\"] + gradients[\"hnxt\"]\n","            gradients[\"a\"] = np.multiply(gradients[\"h\"], (1 - np.square(cc[t])))\n","            gradients[\"U\"] += gradients[\"a\"]@bb[t].T\n","            gradients[\"W\"] += gradients[\"a\"]@cc[t-1].T\n","            gradients[\"b\"] += gradients[\"a\"]\n","            gradients[\"hnxt\"] = self.W.T@gradients[\"a\"]\n","\n","        gradients = {k: gradients[k] for k in gradients if k not in [\"o\", \"h\", \"hnxt\", \"a\"]}\n","        for gr in gradients: gradients[gr] = np.clip(gradients[gr], -5, 5)\n","        he = cc[nt-1]\n","        return gradients, loss, he\n","\n","    def ComputeGradientsNumerically(self, inputs, targets, hp, h, nc=20):\n","        network_params = {\"W\": self.W, \"U\": self.U, \"V\": self.V, \"b\": self.b, \"c\": self.c}\n","        num_gradients  = {\"W\": np.zeros_like(self.W), \"U\": np.zeros_like(self.U),\n","                      \"V\": np.zeros_like(self.V), \"b\": np.zeros_like(self.b),\n","                      \"c\": np.zeros_like(self.c)}\n","\n","        for key in network_params:\n","            for i in range(nc):\n","                prevpar = network_params[key].flat[i] \n","                network_params[key].flat[i] = prevpar + h\n","                _, l1, _ = self.ComputeGradientsAnalytically(inputs, targets, hp)\n","                network_params[key].flat[i] = prevpar - h\n","                _, l2, _ = self.ComputeGradientsAnalytically(inputs, targets, hp)\n","                network_params[key].flat[i] = prevpar \n","                num_gradients[key].flat[i] = (l1 - l2) / (2*h)\n","\n","        return num_gradients\n","\n","\n","    def CheckGradients(self, inputs, targets, hp, nc=20):\n","        analytical_gr, _, _ = self.ComputeGradientsAnalytically(inputs, targets, hp)\n","        numerical_gr = self.ComputeGradientsNumerically(inputs, targets, hp, 1e-5)\n","\n","        err = Texttable()\n","        err_data = [] \n","\n","        # Compare accurate numerical method with analytical estimation of gradient\n","        err_data.append(['Gradient', 'Method',  'Abs Diff Mean [e-06]'])\n","\n","        print(\"Gradient checks:\")\n","        for grad in analytical_gr:\n","            num   = abs(analytical_gr[grad].flat[:nc] - numerical_gr[grad].flat[:nc])\n","            denom = np.asarray([max(abs(a), abs(b)) + 1e-10 for a,b in zip(analytical_gr[grad].flat[:nc],numerical_gr[grad].flat[:nc])])\n","            err_data.append([grad, \"ANL vs NUM\", str(max(num / denom)*100*10*100)])\n","\n","        err.add_rows(err_data)\n","        print(\"Method Comparison: Analytical vs Numerical\")\n","        print(err.draw())      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kAFRsXgjXnVI","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Check Gradients\n","def CompareGradients():\n","  e=0\n","  data = LoadData()\n","  network = RNN_Gradients(data)\n","  hp = np.zeros((network.m, 1))\n","\n","  inputs = [network.getIndFromChar[char] for char in network.data[e:e+network.N]]\n","  targets = [network.getIndFromChar[char] for char in network.data[e+1:e+network.N+1]]\n","\n","  gradients, loss, hp = network.ComputeGradientsAnalytically(inputs, targets, hp)\n","\n","  # Check gradients\n","  network.CheckGradients(inputs, targets, hp)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMgI8qsobom5","colab_type":"text"},"source":["i) The following generates the gradient comparing result (max relative error) that shows that the implemented analytical gradient method is close enough to be regarded as accurate. Within this check, a state sequence of zeros was used as well as hyperparameters of m=100 (hidden state dimensionality) and eta=.1 (learning rate), seq_length=25 and sig=.01. Important to note is that I only check the first initial entries of the gradient matrices for the following resulting max relative errors."]},{"cell_type":"code","metadata":{"id":"fSonyMGjbNKF","colab_type":"code","outputId":"02077f45-f1c4-451b-87e1-265aa90ebdb5","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["CompareGradients()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Gradient checks:\n","Method Comparison: Analytical vs Numerical\n","+----------+------------+----------------------+\n","| Gradient |   Method   | Abs Diff Mean [e-06] |\n","+==========+============+======================+\n","| W        | ANL vs NUM | 1.188                |\n","+----------+------------+----------------------+\n","| U        | ANL vs NUM | 0.059                |\n","+----------+------------+----------------------+\n","| V        | ANL vs NUM | 0.108                |\n","+----------+------------+----------------------+\n","| b        | ANL vs NUM | 0.010                |\n","+----------+------------+----------------------+\n","| c        | ANL vs NUM | 0.001                |\n","+----------+------------+----------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6NNrhR8CM075","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Run Training\n","losses = []\n","def RunTraining():\n","    num_epochs = 9 #ändra till 3 när jag kört för bästa sen\n","    e, n, epoch = 0, 0, 0\n","    data = LoadData()\n","    network = RNN_Gradients(data) \n","    network_params = {\"W\": network.W, \"U\": network.U, \"V\": network.V, \"b\": network.b, \"c\": network.c}\n","    params = {\"W\": np.zeros_like(network.W), \"U\": np.zeros_like(network.U), \"V\": np.zeros_like(network.V), \"b\": np.zeros_like(network.b),\"c\": np.zeros_like(network.c)}\n","\n","    while epoch <= num_epochs and n <= 600000: #ändra till 100,000 när denna är klar\n","        if n == 0 or e >= (len(network.data) - network.N - 1):\n","            hp = np.zeros((network.m, 1))\n","            epoch += 1\n","            e = 0\n","\n","        inputs = [network.getIndFromChar[char] for char in network.data[e:e+network.N]]\n","        targets = [network.getIndFromChar[char] for char in network.data[e+1:e+network.N+1]]\n","        gradients, loss, hp = network.ComputeGradientsAnalytically(inputs, targets, hp)\n","\n","        if n == 0 and epoch == 1: smoothloss = loss\n","        smoothloss = 0.999 * smoothloss + 0.001 * loss\n","\n","        if n % 10000 == 0:\n","            text = network.SynthesizeText(hp, inputs[0], 200)\n","            print('\\nIterations %i, smooth loss: %f \\n %s\\n' % (n, smoothloss, text))\n","\n","        for k in network_params:\n","            params[k] += gradients[k] * gradients[k]\n","            network_params[k] -= network.eta / np.sqrt(params[k] + np.finfo(float).eps) * gradients[k]\n","\n","        e += network.N\n","        n += 1\n","        losses.append(smoothloss)     \n","\n","    text = network.SynthesizeText(hp, inputs[0], 1000)\n","\n","    print('\\nBest performance')\n","    print('\\n %s\\n' % (text))   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TmI2uy8-p0Mx","colab_type":"text"},"source":["iii) Next follows the 200 characters of synthesized text before the first and before every 10,000th update steps when I train for 100,000 update steps. Smooth loss is also displayed\n","\n","iv) Best performance of 1000 characters is also presented (390,000 iterations)"]},{"cell_type":"code","metadata":{"id":"1TXKCcwRb0bd","colab_type":"code","outputId":"d97d7340-48b1-425e-cb1d-8eef57a12e07","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["RunTraining()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Iterations 0, smooth loss: 109.549578 \n"," vwcob1k7j)Z9CAgHKüt}/)MAGiUmC(ZBxDziJ,I:G,PY4v)V6gXucE2MRX20f\n",",\"PL4Ywyyvo^I2DuPE•w,9x;TlIo(GUq/gHüa I,szAAma:EWLL:IQ.Y7:;\tMV0qe2üy6K3pW0xmm? ktdaNJKOwv7ESIA}N.g(FOqFk^KAt9/-yKCnlOiz,ü\tyHl6kcMgoX6AePDG\n","\n","\n","Iterations 10000, smooth loss: 56.221798 \n"," ole. .  Bed harcutea walls thand rey'a sidess cust,\"\n","Pailping hes rtromitheighery as'singe,\" the diand innted to bul gh \"I sulled ase he the foachey ingom out  ojbaen ladminglait Hardoud witistithith \n","\n","\n","Iterations 20000, smooth loss: 52.583628 \n"," ed hay; carkigt late at shes wemped. us -I. buirgaryee tho undo wes's of ton the aling must haflid Fput.  Sbike in \"or cark whinf ouss hagry, roimptod do anming were Cutca oble corciens shough ap sate\n","\n","\n","Iterations 30000, smooth loss: 51.172260 \n"," egrer.\n","\"Wla't nows goft as yough hou thicoutsed oreboning here, . .  This's youg, anrtey?\n","\"Yougit, eatpeder. ... . ... narr, wers thitodid and Cawled dagmsto bagh stich a barkes quast eadbyeperols ack\n","\n","\n","Iterations 40000, smooth loss: 50.165879 \n","  oure drace coughth, to hat he beemory-s uf ald.  OAly,\" sboud hed I frily heen acay outell the gonand . .\n","Harrise thater sevedes Ile of his and and said ongh thisen, thasby stom thene cotchelthing he\n","\n","\n","Iterations 50000, smooth loss: 50.865629 \n","  though his tom of and col turred, a the leone our.\"\n","\"Herly was he and shennaring, frous.\n","\"A'm tols and Cisuve leifore panked,\" to lofs the wis with at wate of ghas dood saraye and ned Cro's ullfows a\n","\n","\n","Iterations 60000, smooth loss: 49.737420 \n","  dern's fext you fout apone, Shese ceed was rooked ame the qual, flit mumoutroad be maidell bapth duned!\"  suterssistes foral, \"Jurt, who hall ataid owaid Dy monca'tind, said The jogwnting.\n","\"Yo have.\n","\n","\n","\n","Iterations 70000, smooth loss: 49.354169 \n"," fon as in wat fiskog off tous them aren.  as?\"\n","\"Mly shought hathey seple the pothing sionk panyardontt hit fating orlling thene Fid.\n","Cand in your he padg ahd, Harrip of ther into had Harryen, masing h\n","\n","\n","Iterations 80000, smooth loss: 47.839903 \n"," omen.  The ary you the berts tereever ston a dighbeered, - said he dowhing..  He meais bow.\n","\"T. wite Dagwrow was seat thats you'rr, rule him twourelfseding at Mrseln't brome houpledong forf, hadley; M\n","\n","\n","Iterations 90000, smooth loss: 48.541875 \n"," nt zerm you's was sold hingr palkack wet us .. ibledy . Whinn a haguntiever lopllay emold lilk.\n","\tquifime icf-ore dastert ... you margeifhangan and heesingy.\n","\"It bantionth rcutt forly hit. That be looo\n","\n","\n","Iterations 100000, smooth loss: 49.408212 \n"," he he the coping are houct treart? The mith ungwe?\"  An rume she hing unging to trenting begmawing wiod be stutmom thes seading dealleth RRenot shail with ben wast coy cosee wat harftey noin a poume t\n","\n","\n","Iterations 110000, smooth loss: 47.565926 \n"," aring hem out thes the besting to jad suring he pray?\" nempont just rif, him -\"I caking his spor tow lit your me dobir vory slef thindy dad was shid the rid in lin to nut be hos as saf ers'h nroward, \n","\n","\n","Iterations 120000, smooth loss: 47.558590 \n","  ne staed. \"Whape.  Gooked as thapllfoniss here menttart feefod hays, the chert one lake fill on pot abetishy dotcriaky.  Nock aly tipar the hall of bomking him You've dyy Saytan to nack dlaconove bic\n","\n","\n","Iterations 130000, smooth loss: 46.752685 \n"," t you peexing and as MoRbeding igh that smopoy the whires that I wish thinthered him sleettry.\"\n","\"Aurer wand, Weduns; so coubl-. . . .\"\n","\"Dumporzerer in the gafore which, it manying everned fiming sotin\n","\n","\n","Iterations 140000, smooth loss: 49.555780 \n"," ed amand on to gobbededofperould of whe limpee the cuplancmarss filun, and - arround ove and of a voingermentt blowp scarting Harry bracely on wonso abder ceer gomriont bedreameled sind himased and hi\n","\n","\n","Iterations 150000, smooth loss: 47.335362 \n"," ow wolt over and fwo cmoly, dingeath intc'ce the Serfuine fatk.\"\n","Free Mere dyour inctoo yto ceyting juthen is again cound fart coment ayand.\n"," he an pelp wing drealt..  My him.  \"Oh had , stand afly an\n","\n","\n","Iterations 160000, smooth loss: 47.318467 \n"," he watle they thriod sitin, it peande fertaron all expaying usas shoy sirn's at've Imrad. \"No itices and timly, sthering waiked of an' Harry. . It a tabout, shad in santo,\" past Roz Herire waure a coy\n","\n","\n","Iterations 170000, smooth loss: 46.430393 \n"," Hour, to bermiss.  Harry nen up wail cortt . . . ...Mord blined ofte were wear, rull, -\"Nother sise Doss to lefor Siltey south has how the come wair fout look heated ay sure abe dost im, is wo know ve\n","\n","\n","Iterations 180000, smooth loss: 47.534713 \n","  couldy of sexor the sumgarny.  Harry, themored youd anfelley coll,\"\n","\"Yeanded has he would. \"I - I cawnt bee Le, horey.\n","Voddnits sourPy.\n","\"Hardy pret vougels Verniseincon Mavey any, trouded now wool fe\n","\n","\n","Iterations 190000, smooth loss: 47.654171 \n","  to enwart.\n","Wermied the got one af litiou.  sairy aghidt the magisins Finked Ixagal tong the him not voiur led yowher knowsat gow es the colf-atace mortisc at outed a tang digione litt avoll, at hil d\n","\n","\n","Iterations 200000, smooth loss: 46.288266 \n"," d, scepile Cuper tinked. Wealling a fiden.  \"Dest,\" said Harry rabcing sile, his caden and devirnss lqulach cleel, waye so You foixting and atcacle'ved Prar you't!\"  Broto sutcaned mystully had Cetwin\n","\n","\n","Iterations 210000, smooth loss: 46.309030 \n"," inkon alle were mearnt but gon had for treselssed. .\n","\"No dapont peem an - \"The him faring was youll deason had eaws, Wer the Citilcuod he the pide walk hims hew, exfere batffinve'd abont, hadf the dob\n","\n","\n","Iterations 220000, smooth loss: 45.458486 \n"," t trisesnenctil?\"  The Pesch.  Heruld.  She lefed tander with Krum.  \"Ke snaired air. . he statest Euss have Votecloured on outeeastol me!\"\n","\"Io bolet, his condotcromfeed thincarow that my just theyer \n","\n","\n","Iterations 230000, smooth loss: 45.702497 \n"," Ron to Mr. Weay, tiks gery by the his in in the's thene retisen to chear.\n","\"What Brom the palle sew watch nivcher sigh?\"\n","It -\"\n","Wearone astepscests as who!\" said He. \"Itun then an timing yougly?\" . in, \n","\n","\n","Iterations 240000, smooth loss: 46.448490 \n"," nthy \"pedeee.  Cet Harry Harry umpaevern, if beson't Dumplackents them you,\" said Harry.  He came hig agay shatping it orcoungecril Hagrys, matk houldcly of seavell him.\n","\t\"That off nors be was and ind\n","\n","\n","Iterations 250000, smooth loss: 46.968253 \n"," landon.\"\n","\"Io fascred mone coming to go got Feloy nas at the aponting ontled newr, I gether.\"\n","Omer disfic at the rork - efat walm and I time, his he sereh, the get the Oglly wan's the his at hisse and \n","\n","\n","Iterations 260000, smooth loss: 45.188710 \n"," Yoo lack letcenbost raightly Harry verp and that a wamed culll.\n","\"Domeand keetore, whit head Pomble, and in the backed. Harrulviy that the surked juct thre?\"\n","He penw storrslesed.  \"I lid morting whis a\n","\n","\n","Iterations 270000, smooth loss: 46.559270 \n"," st. Weame.  \"At are now Andiont he lasleytherst and 'roms,\" said Mlight trew macking and thou this this did Cigdins becobe to pell with for at caseld pucontemp take, fore.  TheRent of das Antsunge the\n","\n","\n","Iterations 280000, smooth loss: 47.020566 \n"," wirting feet indo, the'vor-libled to didores over bugron.  The grottur grorkertifed as upingireston't owh arrsetery upmay they thoutry cerangly spoll, and, look...\" Ron, \"as in -\"\n","\"I'm look, strombler\n","\n","\n","Iterations 290000, smooth loss: 45.623323 \n"," itelal in the Giffor any.\n","Thice comen you whare were the glew laile who disiy secmed got jirst in Mr. Ron bourenss are Hosked by cought you, infur.\n","\"Rudding to Fho's up. Sin pout the chow disnd squrek\n","\n","\n","Iterations 300000, smooth loss: 44.399938 \n"," Harry. \"I sewing then waarow.\n","\"Geemortly monet to them they shay Kriggr tor becous.  Harded.\n","Itil; the star on angon chave Bust in though to theu falled.\n","\"Dulknos waid Ron - -\"\n","Thatre, had lequle care\n","\n","\n","Iterations 310000, smooth loss: 44.505906 \n"," h thut't a sfutres.\"\n","We star Durgle\"fan, mised over on I town lorge a ground he's you of better love.\"\n","\"On I filat itiou. Vold to hive word was as a intpartaed stail-scmelt that ivor, yilt groated on \n","\n","\n","Iterations 320000, smooth loss: 45.841045 \n"," exesp the stryod?\"\n","Very evenychuvion that was hoully said, siveith lial -\"\n","\"Moonst is tho tizad thouges.  Whic molt be whatcing.\n","\"Hadves, he's ceaking laskny a gronch vasire grome it abrud look falled\n","\n","\n","Iterations 330000, smooth loss: 45.313284 \n"," oge a he Hadred being.\n","\tNom to jist of then ouxion's out at tolite lighm, andentcrove fing nace to the didemput fime hen's I've wace offer.\n","\"I yalt . \"We tett's, in should ussid, mollt, lime tunt teve\n","\n","\n","Iterations 340000, smooth loss: 45.590387 \n"," l', his comsiones why a was dy though as theimec, courd goblely stowe eraks couldry bugly giblabby for the dix Harry so filely was thrir theim susped dupp the batter anaugh, an and spaking be tece got\n","\n","\n","Iterations 350000, smooth loss: 45.458497 \n"," brabeld Harry fal his folse the sow strave, ged that and han stilen are narge and file batwen as a mout not!\"  neen didgy, scarl, the was of the Liggize my Vomon could ile he weir te havite, wings had\n","\n","\n","Iterations 360000, smooth loss: 46.547103 \n"," te sure fac up, es wall Mr. Juplely bray, in the strecmed.  The curproget,\" sayt wasly?  . . ... ever bat got they popped of the voiking out intone as the poon's urmars of frolly in pasal and say shad\n","\n","\n","Iterations 370000, smooth loss: 45.467948 \n"," is hain at marove furtonss smich belly calaght of jeas were to grandes on Lorss to gos efor had a dom puror up a got Mr. \"Wared beentle same Dumpting the contents sords must's besens be the spilu bobe\n","\n","\n","Iterations 380000, smooth loss: 45.561363 \n"," l slooked staide fila, no galasily, Haroffly past Rens as they Harry deeked bove rair at and beam, rot to got.\" \"I wammeefer to but door and preasing at whowh frion said atly her witthappe.\n","FO Ron tha\n","\n","\n","Iterations 390000, smooth loss: 44.035467 \n"," and did soriza, ant with the .... . . he top. Harry wiis it, as here of Porking? Pon?\"  said Slep.  \"Hars.  Wee up th Dirry. . . .\n","\"The the off cay?\"  Herrir duikting deen!\" that's Moody, a dark over \n","\n","\n","Best performance\n","\n"," EN Toould the let bork ealfied and Volden at to for arry gad to Gentme-Tlack migh guzaictching and blair.  The Dift Ma. Weimacwar. .\n","\n","HO. MI Dejorver jusk the sind of halm the Goill. . \"You you've's Harry, him ope, there, and ricked to he pesle.  \"I pack.\" Cring of sullsest unveseffinge the mome.  Wen't thet, by, agaits, nos bays thlean though Dumblesenot to hust -\"\n","Mirls, she bach aivom.\"\n","\"Wher exouce?\" said Mngenfoitly bointing my - the - he based clooked!\"\n","\"Beviges and she could of her iflown wert Bovient,\" said Dumbledore.\n","\"What has foughins. He not youch him he be freathing Sorance.\n","\"It'll the mast seamed her and her and Volly, and frinting.\n","It naw her it ass him.\n","\"Yew.\n","\"himing as they one the tapen't to the tain as they dert.\n","\"Pelt mat palt talk keet ......\n","\"Bmint and san ienforls.  he tham no croslict simen he ana.  \"Chem and his hoth.\n","There,\" stutared Pindy was head; mert, there heidy bear them off crevenblerold Dopllened mad is Kermorth, grbented of go whand Hormsgor ouze it -\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KY2cbsA0pB9w","colab_type":"text"},"source":["ii) A graph of the smooth loss function for a longish training run (3 epochs)"]},{"cell_type":"code","metadata":{"id":"dCOjyPorpXCg","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Functions: Smooth Loos Plot\n","def plot():\n","    loss_plot = plt.plot(losses, label=\"training loss\")\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend()\n","    plt.show() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"STXwJCzRpAWG","colab_type":"code","outputId":"f84fe69b-4953-45d3-d3bf-0d2a3166b84a","colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["plot()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}